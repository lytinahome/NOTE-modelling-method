# Regularization

#### When to use regularization?
+ Low resource learning in NLP

## L1 regularization
L1 regularization is equivilent to adding a Laplace Prior.

Sparsity is important:
+ related to model compression: 1) Factorization 2) add sparse prior - Paper: Bayesian Compression for Deep Learning.


## L2 regularization
L2 regularization is equivilent to adding a Gussian Prior.



## Elastic Net

Combine the L1 and L2.

#### How to find the optimal choice of L1 and L2?

Grid Search + Cross Validation.

One can also try the Heuristic Search:
+ Hill Climbing
+ Random Search
+ Generic/Evolutionary Algorithm
+ Bayesian Optimization -- Guassian Process
+ Aut Colony

