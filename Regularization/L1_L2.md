# Regularization

#### When to use regularization?
+ Low resource learning in NLP

## L1 regularization -- LASSO
#### Bayes interpretation  
L1 regularization is equivilent to adding a Laplace Prior.

Sparsity is important:
+ related to model compression: 1) Factorization 2) add sparse prior - Paper: Bayesian Compression for Deep Learning.

#### Loss function of LASSO:
+ \sum||y - Xb||^2 + \lambda ||b_i||_1
   - this is a convex function to b with probability 1 when X is continuous variable.
   - when X is discrete, the solution might be not unique.

#### how to optimize LASSO
+ LARS (least angle regression)
+ coordinate descent

#### degree freedom of LASSO
the number of non-zero coefficients

#### advantage of LASSO
+ do the variable selection automatically.

#### limitations of Lasso
+ the number of feature is bounded by the number of samples. This is because of the KKT limitation
+ the lasso fails to do grouped selection. It tends to select one varaible from a group randomly and ignore the others -- this one can be solved by group Lasso (but you need to preset the group).

## L2 regularization -- Ridge
#### Bayes interpretation
L2 regularization is equivilent to adding a Gussian Prior.

#### Loss function of Ridge:
+ \sum||y - Xb||^2 + \lambda ||b_i||^2
  - strict convex, always has unique solution.

#### degree freedom of Ridge
df = trace(H)
H = X (X^T X + \lambda I)^(-1) X^T

#### advantage of L2 over L1:
+ Removes the limitation on the number of selected variables;
+ Encourages grouping effect;

## Elastic Net
#### Loss function of Elastic Net
Combine the L1 and L2.

#### degree freedom of Elastic Net
df = trace(H)
H = X (X^T X + \lambda I)^(-1) X^T, X is the active set.

#### when to use Elastic Net?
trying to get the best of two world - L1 and L2. another "side effect" is Elastic Net can stabilizes the l1 regularization path.
In practice, the performamce of Elastic Net is better than LASSO or Ridge.

#### application of Elastic Net
+ sparse PCA 
+ support kernel machines

#### How to find the optimal choice of L1 and L2?
Grid Search + Cross Validation.

One can also try the Heuristic Search:
+ Hill Climbing
+ Random Search
+ Generic/Evolutionary Algorithm
+ Bayesian Optimization -- Guassian Process
+ Aut Colony

